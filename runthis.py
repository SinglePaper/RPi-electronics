import os

MODEL_NAME = 'charlie_detect_v4'     # Fill in the name of your model
MIN_SCORE_THRESHOLD = 0.2            # Fill in the threshold for the 

# Directories 
DATA_DIR = os.path.join(os.getcwd(), 'data')
MODELS_DIR = os.path.join(DATA_DIR, 'models')

PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))
PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))

LABEL_FILENAME = 'label_map.pbtxt'
PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))

# Load the model                                                                        

os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging
import tensorflow as tf
from object_detection.utils import label_map_util
from object_detection.utils import config_util
from object_detection.utils import visualization_utils as viz_utils
from object_detection.builders import model_builder

tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)

# Enable GPU dynamic memory allocation
USE_GPU = True   # Set to False if you opt not to use your GPU
if USE_GPU:
    gpus = tf.config.experimental.list_physical_devices('GPU')  
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

# Load pipeline config and build a detection model
configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)
model_config = configs['model']
detection_model = model_builder.build(model_config=model_config, is_training=False)

# Restore checkpoint
ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)
ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()

@tf.function
def detect_fn(image):
    #Detect objects in image

    image, shapes = detection_model.preprocess(image)
    prediction_dict = detection_model.predict(image, shapes)
    detections = detection_model.postprocess(prediction_dict, shapes)

    return detections, prediction_dict, tf.reshape(shapes, [-1])



# Load label map data (for plotting)
# Creates a dictionary that turns predictions of numbers into the string that corresponds with that number.
category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,
                                                                    use_display_name=True)


# Define the video stream
# We will use OpenCV to capture the video stream generated by the webcam. 
import cv2

videocaputure_url = "http://charlie.local:8080/stream/video.mjpeg"  # The URL to the videostream. Use integer numbers to use local cameras instead.
cap = cv2.VideoCapture(videocaputure_url)
cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Set a low buffer to minimize delay later on


# The following code takes a frame from the camera's video feed, runs it through the detection model and then shows the detections that pass the score threshold on the screen.
# If you are curious, you could print "detections['detection_boxes']" to look at the detections and compare them to those shown on the screen.
import numpy as np
from math import floor
import requests
from json import dumps
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry


while True:
    # Read frame from camera
    ret, frame = cap.read()
    image_np = np.array(frame)

    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
    image_np_expanded = np.expand_dims(image_np, axis=0)

    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)
    detections, predictions_dict, shapes = detect_fn(input_tensor)
    # print(detections['detection_boxes'])
    label_id_offset = 1
    image_np_with_detections = image_np.copy()

    viz_utils.visualize_boxes_and_labels_on_image_array(
          image_np_with_detections,
          detections['detection_boxes'][0].numpy(),
          (detections['detection_classes'][0].numpy() + label_id_offset).astype(int),
          detections['detection_scores'][0].numpy(),
          category_index,
          use_normalized_coordinates=True,
          max_boxes_to_draw=25,     # You could change this variable to change the amount of detection boxes drawn on the frame 
          min_score_thresh=MIN_SCORE_THRESHOLD,  # Change thing variable at the start of this file
          agnostic_mode=False)

    # Calculate the location of the center indicator in the middle of the most certain detection that passes the threshold.
    box = detections['detection_boxes'][0][0].numpy() 
    center = ((box[3]-box[1])/2+box[1],(box[2]-box[0])/2+box[0])
    
    # Display output
    if detections['detection_scores'][0][0].numpy()>MIN_SCORE_THRESHOLD:  # If the detection with the highest score passes the threshold
        if abs(center[0]-0.5)>0.1:  # True if the image is off-center by a margin larger than 10% (you can change the 0.1 if your motors are more/less precise)
            color = (0,0,255)  # Red when off-center
            s = requests.Session()
            retry = Retry(connect = 5, backoff_factor = 1)
            adapter = HTTPAdapter(max_retries = retry)
            s.mount('http://', adapter)
            s.keep_alive = False
            if center[0]>0.5:  # If the object is to the right of the robot
                url = 'http://charlie.local/receiver'  # URL that receives POST
                myobj = {"direction": 3, "speed": 1, "AI": 1}   # This will be sent to the RPi, which will then act upon it in receivey.py
                data = dumps(myobj)
                x = s.post(url, json = data)  # Post the JSON to the url
                #print("right")
            elif center[0]<0.5:  # If the object is to the left of the object
                url = 'http://charlie.local/receiver'
                myobj = {"direction": 1, "speed": 1, "AI": 1}
                data = dumps(myobj)
                x = s.post(url, json = data)
                #print("left")
            cap.release()
            cap = cv2.VideoCapture(videocaputure_url) 
        else:
            color = (0,128,0)  # Green if centered
        cv2.circle(image_np_with_detections,(floor(center[0]*frame.shape[1]),floor(center[1]*frame.shape[0])),5,color,-1)  # Draw the dot in the middle on the frame
    cv2.imshow('object detection',  cv2.resize(image_np_with_detections, (800, 600)))   # Show the frame with detections, etc. in the application

    if cv2.waitKey(25) & 0xFF == ord('q'):  # Use key 'q' to quit the application
        break

cap.release()
cv2.destroyAllWindows()